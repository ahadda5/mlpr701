{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUT_DIR = '/tmp/'\n",
    "NUM_WORKERS = 16\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ADV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashraf.haddad/.conda/envs/rob/lib/python3.9/site-packages/robustness/train.py:24: UserWarning: Could not import amp.\n",
      "  warnings.warn('Could not import amp.')\n"
     ]
    }
   ],
   "source": [
    "from robustness import model_utils, datasets, train, defaults\n",
    "from robustness.datasets import CINIC#CIFAR#FashionMnist\n",
    "from robustness import data_augmentation as da\n",
    "import torch \n",
    "import torchvision.datasets\n",
    "\n",
    "#import sys\n",
    "#sys.path.append('/home/u21010246/mlpr/venv/lib/python3.8/site-packages/robustness')\n",
    "\n",
    "# We use cox (http://github.com/MadryLab/cox) to log, store and analyze\n",
    "# results. Read more at https//cox.readthedocs.io.\n",
    "from cox.utils import Parameters\n",
    "import cox.store\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing dataset cinic..\n"
     ]
    }
   ],
   "source": [
    "# Hard-coded dataset, architecture, batch size, workers\n",
    "ds = CINIC('/tmp/cinic') # CIFAR('/tmp')\n",
    "m, _ = model_utils.make_and_restore_model(arch='resnet50', dataset=ds)  #same model as std training \n",
    "train_loader, val_loader = ds.make_loaders(batch_size=BATCH_SIZE, workers=NUM_WORKERS)\n",
    "\n",
    "#wget http://data.csail.mit.edu/places/places365/places365standard_easyformat.tar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a cox store for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in: /tmp/db5d66ec-222a-4f7b-aa2b-86da20d95cf9\n"
     ]
    }
   ],
   "source": [
    "# Create a cox store for logging  , later useful for tensboard to visualize the accuracy curve over epochs\n",
    "out_store = cox.store.Store(OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up training arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hard-coded base parameters\n",
    "train_kwargs = {\n",
    "    'out_dir': \"train_out\",\n",
    "    'adv_train': 1,\n",
    "    'constraint': '2',\n",
    "    'eps': 0.5,\n",
    "    'attack_lr': 0.1,\n",
    "    'attack_steps': 7,\n",
    "    'epochs': 200\n",
    "}\n",
    "train_args = Parameters(train_kwargs)\n",
    "\n",
    "# Fill whatever parameters are missing from the defaults\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.TRAINING_ARGS, CINIC)\n",
    "train_args = defaults.check_and_fill_args(train_args,\n",
    "                        defaults.PGD_ARGS, CINIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:0 | Loss 2.5480 | AdvPrec1 15.230 | AdvPrec5 59.977 | Reg term: 0.0 \n",
      "Val Epoch:0 | Loss 2.1089 | NatPrec1 21.974 | NatPrec5 74.239 | Reg term: 0.0 ||\n",
      "Val Epoch:0 | Loss 2.1795 | AdvPrec1 18.839 | AdvPrec5 67.956 | Reg term: 0.0 ||\n",
      "Train Epoch:1 | Loss 2.1525 | AdvPrec1 18.893 | AdvPrec5 69.399 | Reg term: 0.0 \n",
      "Train Epoch:2 | Loss 2.0387 | AdvPrec1 22.643 | AdvPrec5 76.241 | Reg term: 0.0 \n",
      "Train Epoch:3 | Loss 1.9546 | AdvPrec1 25.924 | AdvPrec5 80.148 | Reg term: 0.0 \n",
      "Train Epoch:4 | Loss 1.8887 | AdvPrec1 28.414 | AdvPrec5 82.617 | Reg term: 0.0 \n",
      "Train Epoch:5 | Loss 1.8327 | AdvPrec1 30.650 | AdvPrec5 84.324 | Reg term: 0.0 \n",
      "Val Epoch:5 | Loss 1.5644 | NatPrec1 42.577 | NatPrec5 90.860 | Reg term: 0.0 ||\n",
      "Val Epoch:5 | Loss 1.8153 | AdvPrec1 30.802 | AdvPrec5 85.190 | Reg term: 0.0 ||\n",
      "Train Epoch:6 | Loss 1.7809 | AdvPrec1 32.760 | AdvPrec5 85.717 | Reg term: 0.0 \n",
      "Train Epoch:7 | Loss 1.7405 | AdvPrec1 34.258 | AdvPrec5 86.703 | Reg term: 0.0 \n",
      "Train Epoch:8 | Loss 1.7102 | AdvPrec1 35.119 | AdvPrec5 87.432 | Reg term: 0.0 \n",
      "Train Epoch:9 | Loss 1.6824 | AdvPrec1 36.372 | AdvPrec5 88.066 | Reg term: 0.0 \n",
      "Train Epoch:10 | Loss 1.6594 | AdvPrec1 37.284 | AdvPrec5 88.531 | Reg term: 0.0\n",
      "Val Epoch:10 | Loss 1.3273 | NatPrec1 51.520 | NatPrec5 93.620 | Reg term: 0.0 |\n",
      "Val Epoch:10 | Loss 1.6762 | AdvPrec1 37.293 | AdvPrec5 88.458 | Reg term: 0.0 |\n",
      "Train Epoch:11 | Loss 1.6412 | AdvPrec1 37.781 | AdvPrec5 88.890 | Reg term: 0.0\n",
      "Train Epoch:12 | Loss 1.6201 | AdvPrec1 38.940 | AdvPrec5 89.411 | Reg term: 0.0\n",
      "Train Epoch:13 | Loss 1.6095 | AdvPrec1 39.358 | AdvPrec5 89.400 | Reg term: 0.0\n",
      "Train Epoch:14 | Loss 1.5966 | AdvPrec1 39.713 | AdvPrec5 89.787 | Reg term: 0.0\n",
      "Train Epoch:15 | Loss 1.5859 | AdvPrec1 40.050 | AdvPrec5 89.877 | Reg term: 0.0\n",
      "Val Epoch:15 | Loss 1.2521 | NatPrec1 54.319 | NatPrec5 94.302 | Reg term: 0.0 |\n",
      "Val Epoch:15 | Loss 1.6730 | AdvPrec1 38.242 | AdvPrec5 89.319 | Reg term: 0.0 |\n",
      "Train Epoch:16 | Loss 1.5757 | AdvPrec1 40.708 | AdvPrec5 90.160 | Reg term: 0.0\n",
      "Train Epoch:17 | Loss 1.5685 | AdvPrec1 41.002 | AdvPrec5 90.270 | Reg term: 0.0\n",
      "Train Epoch:18 | Loss 1.5572 | AdvPrec1 41.417 | AdvPrec5 90.439 | Reg term: 0.0\n",
      "Train Epoch:19 | Loss 1.5524 | AdvPrec1 41.512 | AdvPrec5 90.644 | Reg term: 0.0\n",
      "Train Epoch:20 | Loss 1.5454 | AdvPrec1 41.746 | AdvPrec5 90.472 | Reg term: 0.0\n",
      "Val Epoch:20 | Loss 1.1925 | NatPrec1 56.711 | NatPrec5 95.038 | Reg term: 0.0 |\n",
      "Val Epoch:20 | Loss 1.6026 | AdvPrec1 39.707 | AdvPrec5 89.951 | Reg term: 0.0 |\n",
      "Train Epoch:21 | Loss 1.5415 | AdvPrec1 41.981 | AdvPrec5 90.556 | Reg term: 0.0\n",
      "Train Epoch:22 | Loss 1.5330 | AdvPrec1 42.313 | AdvPrec5 90.800 | Reg term: 0.0\n",
      "Train Epoch:23 | Loss 1.5254 | AdvPrec1 42.647 | AdvPrec5 90.782 | Reg term: 0.0\n",
      "Train Epoch:24 | Loss 1.5252 | AdvPrec1 42.662 | AdvPrec5 90.884 | Reg term: 0.0\n",
      "Train Epoch:25 | Loss 1.5213 | AdvPrec1 42.790 | AdvPrec5 90.961 | Reg term: 0.0\n",
      "Val Epoch:25 | Loss 1.1658 | NatPrec1 58.132 | NatPrec5 95.679 | Reg term: 0.0 |\n",
      "Val Epoch:25 | Loss 1.5990 | AdvPrec1 39.370 | AdvPrec5 90.998 | Reg term: 0.0 |\n",
      "Train Epoch:26 | Loss 1.5188 | AdvPrec1 42.986 | AdvPrec5 91.054 | Reg term: 0.0\n",
      "Train Epoch:27 | Loss 1.5159 | AdvPrec1 43.033 | AdvPrec5 91.074 | Reg term: 0.0\n",
      "Train Epoch:28 | Loss 1.5098 | AdvPrec1 43.268 | AdvPrec5 91.059 | Reg term: 0.0\n",
      "Train Epoch:29 | Loss 1.5060 | AdvPrec1 43.373 | AdvPrec5 91.214 | Reg term: 0.0\n",
      "Train Epoch:30 | Loss 1.5063 | AdvPrec1 43.526 | AdvPrec5 91.198 | Reg term: 0.0\n",
      "Val Epoch:30 | Loss 1.1459 | NatPrec1 60.788 | NatPrec5 95.261 | Reg term: 0.0 |\n",
      "Val Epoch:30 | Loss 1.5771 | AdvPrec1 42.153 | AdvPrec5 89.740 | Reg term: 0.0 |\n",
      "Train Epoch:31 | Loss 1.5021 | AdvPrec1 43.530 | AdvPrec5 91.356 | Reg term: 0.0\n",
      "Train Epoch:32 | Loss 1.5000 | AdvPrec1 43.721 | AdvPrec5 91.262 | Reg term: 0.0\n",
      "Train Epoch:33 | Loss 1.4974 | AdvPrec1 43.639 | AdvPrec5 91.320 | Reg term: 0.0\n",
      "Train Epoch:34 | Loss 1.4943 | AdvPrec1 43.983 | AdvPrec5 91.370 | Reg term: 0.0\n",
      "Train Epoch:35 | Loss 1.4890 | AdvPrec1 44.136 | AdvPrec5 91.504 | Reg term: 0.0\n",
      "Val Epoch:35 | Loss 1.1763 | NatPrec1 57.359 | NatPrec5 95.540 | Reg term: 0.0 |\n",
      "Val Epoch:35 | Loss 1.6056 | AdvPrec1 39.870 | AdvPrec5 90.676 | Reg term: 0.0 |\n",
      "Train Epoch:36 | Loss 1.4860 | AdvPrec1 44.012 | AdvPrec5 91.518 | Reg term: 0.0\n",
      "Train Epoch:37 | Loss 1.4867 | AdvPrec1 44.227 | AdvPrec5 91.396 | Reg term: 0.0\n",
      "Train Epoch:38 | Loss 1.4841 | AdvPrec1 44.316 | AdvPrec5 91.574 | Reg term: 0.0\n",
      "Train Epoch:39 | Loss 1.4827 | AdvPrec1 44.423 | AdvPrec5 91.536 | Reg term: 0.0\n",
      "Train Epoch:40 | Loss 1.4861 | AdvPrec1 44.064 | AdvPrec5 91.520 | Reg term: 0.0\n",
      "Val Epoch:40 | Loss 1.0807 | NatPrec1 60.820 | NatPrec5 96.160 | Reg term: 0.0 |\n",
      "Val Epoch:40 | Loss 1.5302 | AdvPrec1 42.374 | AdvPrec5 91.952 | Reg term: 0.0 |\n",
      "Train Epoch:41 | Loss 1.4771 | AdvPrec1 44.582 | AdvPrec5 91.588 | Reg term: 0.0\n",
      "Train Epoch:42 | Loss 1.4769 | AdvPrec1 44.556 | AdvPrec5 91.549 | Reg term: 0.0\n",
      "Train Epoch:43 | Loss 1.4750 | AdvPrec1 44.661 | AdvPrec5 91.708 | Reg term: 0.0\n",
      "Train Epoch:44 | Loss 1.4724 | AdvPrec1 44.760 | AdvPrec5 91.574 | Reg term: 0.0\n",
      "Train Epoch:45 | Loss 1.4725 | AdvPrec1 44.612 | AdvPrec5 91.690 | Reg term: 0.0\n",
      "Val Epoch:45 | Loss 1.0743 | NatPrec1 62.616 | NatPrec5 96.118 | Reg term: 0.0 |\n",
      "Val Epoch:45 | Loss 1.4939 | AdvPrec1 43.966 | AdvPrec5 91.596 | Reg term: 0.0 |\n",
      "Train Epoch:46 | Loss 1.4692 | AdvPrec1 44.904 | AdvPrec5 91.736 | Reg term: 0.0\n",
      "Train Epoch:47 | Loss 1.4691 | AdvPrec1 44.939 | AdvPrec5 91.778 | Reg term: 0.0\n",
      "Train Epoch:48 | Loss 1.4709 | AdvPrec1 44.848 | AdvPrec5 91.651 | Reg term: 0.0\n",
      "Train Epoch:49 | Loss 1.4673 | AdvPrec1 44.881 | AdvPrec5 91.759 | Reg term: 0.0\n",
      "Train Epoch:50 | Loss 1.3066 | AdvPrec1 50.836 | AdvPrec5 93.776 | Reg term: 0.0\n",
      "Val Epoch:50 | Loss 0.8371 | NatPrec1 71.340 | NatPrec5 97.670 | Reg term: 0.0 |\n",
      "Val Epoch:50 | Loss 1.2648 | AdvPrec1 52.334 | AdvPrec5 94.492 | Reg term: 0.0 |\n",
      "Train Epoch:51 | Loss 1.2534 | AdvPrec1 52.727 | AdvPrec5 94.402 | Reg term: 0.0\n",
      "Train Epoch:52 | Loss 1.2329 | AdvPrec1 53.321 | AdvPrec5 94.726 | Reg term: 0.0\n",
      "Train Epoch:53 | Loss 1.2155 | AdvPrec1 54.086 | AdvPrec5 94.866 | Reg term: 0.0\n",
      "Train Epoch:54 | Loss 1.2029 | AdvPrec1 54.642 | AdvPrec5 94.999 | Reg term: 0.0\n",
      "Train Epoch:55 | Loss 1.1938 | AdvPrec1 54.979 | AdvPrec5 95.163 | Reg term: 0.0\n",
      "Val Epoch:55 | Loss 0.7777 | NatPrec1 73.068 | NatPrec5 97.936 | Reg term: 0.0 |\n",
      "Val Epoch:55 | Loss 1.2401 | AdvPrec1 53.357 | AdvPrec5 94.876 | Reg term: 0.0 |\n",
      "Train Epoch:56 | Loss 1.1837 | AdvPrec1 55.192 | AdvPrec5 95.139 | Reg term: 0.0\n",
      "Train Epoch:57 | Loss 1.1717 | AdvPrec1 55.584 | AdvPrec5 95.331 | Reg term: 0.0\n",
      "Train Epoch:58 | Loss 1.1675 | AdvPrec1 55.953 | AdvPrec5 95.449 | Reg term: 0.0\n",
      "Train Epoch:59 | Loss 1.1621 | AdvPrec1 55.884 | AdvPrec5 95.503 | Reg term: 0.0\n",
      "Train Epoch:60 | Loss 1.1534 | AdvPrec1 56.276 | AdvPrec5 95.560 | Reg term: 0.0\n",
      "Val Epoch:60 | Loss 0.7596 | NatPrec1 73.741 | NatPrec5 98.010 | Reg term: 0.0 |\n",
      "Val Epoch:60 | Loss 1.2632 | AdvPrec1 52.989 | AdvPrec5 94.761 | Reg term: 0.0 |\n",
      "Train Epoch:61 | Loss 1.1482 | AdvPrec1 56.472 | AdvPrec5 95.630 | Reg term: 0.0\n",
      "Train Epoch:62 | Loss 1.1437 | AdvPrec1 56.682 | AdvPrec5 95.697 | Reg term: 0.0\n",
      "Train Epoch:63 | Loss 1.1329 | AdvPrec1 56.850 | AdvPrec5 95.893 | Reg term: 0.0\n",
      "Train Epoch:64 | Loss 1.1309 | AdvPrec1 57.063 | AdvPrec5 95.804 | Reg term: 0.0\n",
      "Train Epoch:65 | Loss 1.1249 | AdvPrec1 57.128 | AdvPrec5 95.939 | Reg term: 0.0\n",
      "Val Epoch:65 | Loss 0.7668 | NatPrec1 73.213 | NatPrec5 98.022 | Reg term: 0.0 |\n",
      "Val Epoch:65 | Loss 1.2718 | AdvPrec1 52.347 | AdvPrec5 94.653 | Reg term: 0.0 |\n",
      "Train Epoch:66 | Loss 1.1170 | AdvPrec1 57.397 | AdvPrec5 96.096 | Reg term: 0.0\n",
      "Train Epoch:67 | Loss 1.1144 | AdvPrec1 57.491 | AdvPrec5 96.083 | Reg term: 0.0\n",
      "Train Epoch:68 | Loss 1.1072 | AdvPrec1 57.751 | AdvPrec5 96.103 | Reg term: 0.0\n",
      "Train Epoch:69 | Loss 1.1004 | AdvPrec1 57.846 | AdvPrec5 96.190 | Reg term: 0.0\n",
      "Train Epoch:70 | Loss 1.0956 | AdvPrec1 58.104 | AdvPrec5 96.267 | Reg term: 0.0\n",
      "Val Epoch:70 | Loss 0.7836 | NatPrec1 72.982 | NatPrec5 97.861 | Reg term: 0.0 |\n",
      "Val Epoch:70 | Loss 1.3350 | AdvPrec1 51.123 | AdvPrec5 94.196 | Reg term: 0.0 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:71 | Loss 1.0879 | AdvPrec1 58.301 | AdvPrec5 96.380 | Reg term: 0.0\n",
      "Train Epoch:72 | Loss 1.0806 | AdvPrec1 58.408 | AdvPrec5 96.447 | Reg term: 0.0\n",
      "Train Epoch:73 | Loss 1.0790 | AdvPrec1 58.540 | AdvPrec5 96.399 | Reg term: 0.0\n",
      "Train Epoch:74 | Loss 1.0703 | AdvPrec1 59.040 | AdvPrec5 96.478 | Reg term: 0.0\n",
      "Train Epoch:75 | Loss 1.0652 | AdvPrec1 59.053 | AdvPrec5 96.520 | Reg term: 0.0\n",
      "Val Epoch:75 | Loss 0.7560 | NatPrec1 73.508 | NatPrec5 97.918 | Reg term: 0.0 |\n",
      "Val Epoch:75 | Loss 1.3188 | AdvPrec1 52.053 | AdvPrec5 94.373 | Reg term: 0.0 |\n",
      "Train Epoch:76 | Loss 1.0618 | AdvPrec1 59.153 | AdvPrec5 96.631 | Reg term: 0.0\n",
      "Train Epoch:77 | Loss 1.0490 | AdvPrec1 59.646 | AdvPrec5 96.757 | Reg term: 0.0\n",
      "Train Epoch:78 | Loss 1.0460 | AdvPrec1 59.734 | AdvPrec5 96.732 | Reg term: 0.0\n",
      "Train Epoch:79 | Loss 1.0385 | AdvPrec1 59.988 | AdvPrec5 96.891 | Reg term: 0.0\n",
      "Train Epoch:80 | Loss 1.0337 | AdvPrec1 60.076 | AdvPrec5 96.942 | Reg term: 0.0\n",
      "Val Epoch:80 | Loss 0.7579 | NatPrec1 73.906 | NatPrec5 98.009 | Reg term: 0.0 |\n",
      "Val Epoch:80 | Loss 1.3573 | AdvPrec1 50.644 | AdvPrec5 94.191 | Reg term: 0.0 |\n",
      "Train Epoch:81 | Loss 1.0273 | AdvPrec1 60.220 | AdvPrec5 96.947 | Reg term: 0.0\n",
      "Train Epoch:82 | Loss 1.0212 | AdvPrec1 60.468 | AdvPrec5 97.097 | Reg term: 0.0\n",
      "Train Epoch:83 | Loss 1.0152 | AdvPrec1 60.612 | AdvPrec5 97.089 | Reg term: 0.0\n",
      "Train Epoch:84 | Loss 1.0085 | AdvPrec1 60.850 | AdvPrec5 97.164 | Reg term: 0.0\n",
      "Train Epoch:85 | Loss 1.0076 | AdvPrec1 60.944 | AdvPrec5 97.186 | Reg term: 0.0\n",
      "Val Epoch:85 | Loss 0.7346 | NatPrec1 74.607 | NatPrec5 97.967 | Reg term: 0.0 |\n",
      "Val Epoch:85 | Loss 1.4084 | AdvPrec1 50.589 | AdvPrec5 94.087 | Reg term: 0.0 |\n",
      "Train Epoch:86 | Loss 0.9987 | AdvPrec1 61.127 | AdvPrec5 97.260 | Reg term: 0.0\n",
      "Train Epoch:87 | Loss 0.9936 | AdvPrec1 61.494 | AdvPrec5 97.301 | Reg term: 0.0\n",
      "Train Epoch:88 | Loss 0.9854 | AdvPrec1 61.557 | AdvPrec5 97.394 | Reg term: 0.0\n",
      "Train Epoch:89 | Loss 0.9845 | AdvPrec1 61.650 | AdvPrec5 97.400 | Reg term: 0.0\n",
      "Train Epoch:90 | Loss 0.9730 | AdvPrec1 61.954 | AdvPrec5 97.562 | Reg term: 0.0\n",
      "Val Epoch:90 | Loss 0.7425 | NatPrec1 74.460 | NatPrec5 97.996 | Reg term: 0.0 |\n",
      "Val Epoch:90 | Loss 1.3885 | AdvPrec1 51.067 | AdvPrec5 94.057 | Reg term: 0.0 |\n",
      "Train Epoch:91 | Loss 0.9691 | AdvPrec1 62.117 | AdvPrec5 97.488 | Reg term: 0.0\n",
      "Train Epoch:92 | Loss 0.9653 | AdvPrec1 62.373 | AdvPrec5 97.593 | Reg term: 0.0\n",
      "Train Epoch:93 | Loss 0.9546 | AdvPrec1 62.586 | AdvPrec5 97.682 | Reg term: 0.0\n",
      "Train Epoch:94 | Loss 0.9490 | AdvPrec1 62.833 | AdvPrec5 97.718 | Reg term: 0.0\n",
      "Train Epoch:95 | Loss 0.9425 | AdvPrec1 63.012 | AdvPrec5 97.757 | Reg term: 0.0\n",
      "Val Epoch:95 | Loss 0.7349 | NatPrec1 74.726 | NatPrec5 97.997 | Reg term: 0.0 |\n",
      "Val Epoch:95 | Loss 1.4643 | AdvPrec1 50.258 | AdvPrec5 93.973 | Reg term: 0.0 |\n",
      "Train Epoch:96 | Loss 0.9400 | AdvPrec1 63.268 | AdvPrec5 97.776 | Reg term: 0.0\n",
      "Train Epoch:97 | Loss 0.9336 | AdvPrec1 63.174 | AdvPrec5 97.883 | Reg term: 0.0\n",
      "Train Epoch:98 | Loss 0.9323 | AdvPrec1 63.448 | AdvPrec5 97.850 | Reg term: 0.0\n",
      "Train Epoch:99 | Loss 0.9269 | AdvPrec1 63.586 | AdvPrec5 97.957 | Reg term: 0.0\n",
      "Train Epoch:100 | Loss 0.7517 | AdvPrec1 70.288 | AdvPrec5 98.810 | Reg term: 0.\n",
      "Val Epoch:100 | Loss 0.6589 | NatPrec1 77.390 | NatPrec5 98.219 | Reg term: 0.0 \n",
      "Val Epoch:100 | Loss 1.4983 | AdvPrec1 52.748 | AdvPrec5 94.513 | Reg term: 0.0 \n",
      "Train Epoch:101 | Loss 0.6754 | AdvPrec1 73.022 | AdvPrec5 99.144 | Reg term: 0.\n",
      "Train Epoch:102 | Loss 0.6420 | AdvPrec1 74.130 | AdvPrec5 99.237 | Reg term: 0.\n",
      "Train Epoch:103 | Loss 0.6215 | AdvPrec1 75.018 | AdvPrec5 99.304 | Reg term: 0.\n",
      "Train Epoch:104 | Loss 0.6019 | AdvPrec1 75.502 | AdvPrec5 99.423 | Reg term: 0.\n",
      "Train Epoch:105 | Loss 0.5847 | AdvPrec1 76.240 | AdvPrec5 99.449 | Reg term: 0.\n",
      "Val Epoch:105 | Loss 0.7105 | NatPrec1 77.338 | NatPrec5 98.091 | Reg term: 0.0 \n",
      "Val Epoch:105 | Loss 1.8625 | AdvPrec1 50.777 | AdvPrec5 94.134 | Reg term: 0.0 \n",
      "Train Epoch:106 | Loss 0.5685 | AdvPrec1 76.759 | AdvPrec5 99.497 | Reg term: 0.\n",
      "Train Epoch:107 | Loss 0.5543 | AdvPrec1 77.371 | AdvPrec5 99.502 | Reg term: 0.\n",
      "Train Epoch:108 | Loss 0.5430 | AdvPrec1 77.934 | AdvPrec5 99.559 | Reg term: 0.\n",
      "Train Epoch:109 | Loss 0.5314 | AdvPrec1 78.191 | AdvPrec5 99.561 | Reg term: 0.\n",
      "Train Epoch:110 | Loss 0.5166 | AdvPrec1 78.976 | AdvPrec5 99.576 | Reg term: 0.\n",
      "Val Epoch:110 | Loss 0.7671 | NatPrec1 76.611 | NatPrec5 97.980 | Reg term: 0.0 \n",
      "Val Epoch:110 | Loss 2.0704 | AdvPrec1 49.967 | AdvPrec5 93.788 | Reg term: 0.0 \n",
      "Train Epoch:111 | Loss 0.5100 | AdvPrec1 79.081 | AdvPrec5 99.628 | Reg term: 0.\n",
      "Train Epoch:112 | Loss 0.4982 | AdvPrec1 79.561 | AdvPrec5 99.679 | Reg term: 0.\n",
      "Train Epoch:113 | Loss 0.4876 | AdvPrec1 79.918 | AdvPrec5 99.702 | Reg term: 0.\n",
      "Train Epoch:114 | Loss 0.4772 | AdvPrec1 80.406 | AdvPrec5 99.681 | Reg term: 0.\n",
      "Train Epoch:115 | Loss 0.4700 | AdvPrec1 80.640 | AdvPrec5 99.684 | Reg term: 0.\n",
      "Val Epoch:115 | Loss 0.7994 | NatPrec1 76.723 | NatPrec5 97.967 | Reg term: 0.0 \n",
      "Val Epoch:115 | Loss 2.2695 | AdvPrec1 48.884 | AdvPrec5 93.828 | Reg term: 0.0 \n",
      "Train Epoch:116 | Loss 0.4630 | AdvPrec1 80.851 | AdvPrec5 99.746 | Reg term: 0.\n",
      "Train Epoch:117 | Loss 0.4549 | AdvPrec1 81.418 | AdvPrec5 99.743 | Reg term: 0.\n",
      "Train Epoch:118 | Loss 0.4452 | AdvPrec1 81.742 | AdvPrec5 99.776 | Reg term: 0.\n",
      "Train Epoch:119 | Loss 0.4401 | AdvPrec1 81.896 | AdvPrec5 99.784 | Reg term: 0.\n",
      "Train Epoch:120 | Loss 0.4337 | AdvPrec1 82.064 | AdvPrec5 99.783 | Reg term: 0.\n",
      "Val Epoch:120 | Loss 0.8742 | NatPrec1 76.238 | NatPrec5 97.779 | Reg term: 0.0 \n",
      "Val Epoch:120 | Loss 2.5106 | AdvPrec1 48.128 | AdvPrec5 93.324 | Reg term: 0.0 \n",
      "Train Epoch:121 | Loss 0.4285 | AdvPrec1 82.239 | AdvPrec5 99.781 | Reg term: 0.\n",
      "Train Epoch:122 | Loss 0.4225 | AdvPrec1 82.518 | AdvPrec5 99.768 | Reg term: 0.\n",
      "Train Epoch:123 | Loss 0.4144 | AdvPrec1 82.853 | AdvPrec5 99.799 | Reg term: 0.\n",
      "Train Epoch:124 | Loss 0.4069 | AdvPrec1 83.168 | AdvPrec5 99.800 | Reg term: 0.\n",
      "Train Epoch:125 | Loss 0.4063 | AdvPrec1 83.217 | AdvPrec5 99.808 | Reg term: 0.\n",
      "Val Epoch:125 | Loss 0.8944 | NatPrec1 76.183 | NatPrec5 97.738 | Reg term: 0.0 \n",
      "Val Epoch:125 | Loss 2.6281 | AdvPrec1 47.624 | AdvPrec5 93.257 | Reg term: 0.0 \n",
      "Train Epoch:126 | Loss 0.3958 | AdvPrec1 83.649 | AdvPrec5 99.812 | Reg term: 0.\n",
      "Train Epoch:127 | Loss 0.3928 | AdvPrec1 83.780 | AdvPrec5 99.827 | Reg term: 0.\n",
      "Train Epoch:128 | Loss 0.3899 | AdvPrec1 83.914 | AdvPrec5 99.823 | Reg term: 0.\n",
      "Train Epoch:129 | Loss 0.3794 | AdvPrec1 84.317 | AdvPrec5 99.850 | Reg term: 0.\n",
      "Train Epoch:130 | Loss 0.3756 | AdvPrec1 84.656 | AdvPrec5 99.853 | Reg term: 0.\n",
      "Val Epoch:130 | Loss 0.9182 | NatPrec1 75.822 | NatPrec5 97.813 | Reg term: 0.0 \n",
      "Val Epoch:130 | Loss 2.8026 | AdvPrec1 46.741 | AdvPrec5 93.241 | Reg term: 0.0 \n",
      "Train Epoch:131 | Loss 0.3754 | AdvPrec1 84.477 | AdvPrec5 99.874 | Reg term: 0.\n",
      "Train Epoch:132 | Loss 0.3679 | AdvPrec1 84.764 | AdvPrec5 99.842 | Reg term: 0.\n",
      "Train Epoch:133 | Loss 0.3632 | AdvPrec1 84.936 | AdvPrec5 99.892 | Reg term: 0.\n",
      "Train Epoch:134 | Loss 0.3622 | AdvPrec1 85.008 | AdvPrec5 99.864 | Reg term: 0.\n",
      "Train Epoch:135 | Loss 0.3538 | AdvPrec1 85.452 | AdvPrec5 99.882 | Reg term: 0.\n",
      "Val Epoch:135 | Loss 0.9912 | NatPrec1 75.548 | NatPrec5 97.756 | Reg term: 0.0 \n",
      "Val Epoch:135 | Loss 3.0001 | AdvPrec1 46.022 | AdvPrec5 93.370 | Reg term: 0.0 \n",
      "Train Epoch:136 | Loss 0.3628 | AdvPrec1 85.149 | AdvPrec5 99.858 | Reg term: 0.\n",
      "Train Epoch:137 | Loss 0.3473 | AdvPrec1 85.789 | AdvPrec5 99.884 | Reg term: 0.\n",
      "Train Epoch:138 | Loss 0.3440 | AdvPrec1 85.881 | AdvPrec5 99.884 | Reg term: 0.\n",
      "Train Epoch:139 | Loss 0.3417 | AdvPrec1 85.900 | AdvPrec5 99.891 | Reg term: 0.\n",
      "Train Epoch:140 | Loss 0.3332 | AdvPrec1 86.243 | AdvPrec5 99.899 | Reg term: 0.\n",
      "Val Epoch:140 | Loss 0.9871 | NatPrec1 75.634 | NatPrec5 97.716 | Reg term: 0.0 \n",
      "Val Epoch:140 | Loss 3.0315 | AdvPrec1 45.956 | AdvPrec5 93.080 | Reg term: 0.0 \n",
      "Train Epoch:141 | Loss 0.3382 | AdvPrec1 85.994 | AdvPrec5 99.916 | Reg term: 0.\n",
      "Train Epoch:142 | Loss 0.3335 | AdvPrec1 86.180 | AdvPrec5 99.880 | Reg term: 0.\n",
      "Train Epoch:143 | Loss 0.3259 | AdvPrec1 86.624 | AdvPrec5 99.910 | Reg term: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train Epoch:144 | Loss 0.3256 | AdvPrec1 86.542 | AdvPrec5 99.902 | Reg term: 0.\n",
      "Train Epoch:145 | Loss 0.3211 | AdvPrec1 86.790 | AdvPrec5 99.899 | Reg term: 0.\n",
      "Val Epoch:145 | Loss 1.0268 | NatPrec1 75.467 | NatPrec5 97.567 | Reg term: 0.0 \n",
      "Val Epoch:145 | Loss 3.0830 | AdvPrec1 46.242 | AdvPrec5 92.583 | Reg term: 0.0 \n",
      "Train Epoch:146 | Loss 0.3139 | AdvPrec1 87.051 | AdvPrec5 99.934 | Reg term: 0.\n",
      "Train Epoch:147 | Loss 0.3213 | AdvPrec1 86.839 | AdvPrec5 99.898 | Reg term: 0.\n",
      "Train Epoch:148 | Loss 0.3135 | AdvPrec1 87.197 | AdvPrec5 99.913 | Reg term: 0.\n",
      "Train Epoch:149 | Loss 0.3086 | AdvPrec1 87.298 | AdvPrec5 99.914 | Reg term: 0.\n",
      "Train Epoch:150 | Loss 0.2389 | AdvPrec1 90.406 | AdvPrec5 99.951 | Reg term: 0.\n",
      "Val Epoch:150 | Loss 1.0145 | NatPrec1 76.060 | NatPrec5 97.668 | Reg term: 0.0 \n",
      "Val Epoch:150 | Loss 3.2278 | AdvPrec1 45.881 | AdvPrec5 92.768 | Reg term: 0.0 \n",
      "Train Epoch:151 | Loss 0.2125 | AdvPrec1 91.551 | AdvPrec5 99.959 | Reg term: 0.\n",
      "Train Epoch:152 | Loss 0.1974 | AdvPrec1 92.071 | AdvPrec5 99.969 | Reg term: 0.\n",
      "Train Epoch:153 | Loss 0.1888 | AdvPrec1 92.459 | AdvPrec5 99.972 | Reg term: 0.\n",
      "Train Epoch:154 | Loss 0.1856 | AdvPrec1 92.676 | AdvPrec5 99.961 | Reg term: 0.\n",
      "Train Epoch:155 | Loss 0.1802 | AdvPrec1 92.789 | AdvPrec5 99.974 | Reg term: 0.\n",
      "Val Epoch:155 | Loss 1.0533 | NatPrec1 76.162 | NatPrec5 97.646 | Reg term: 0.0 \n",
      "Val Epoch:155 | Loss 3.4458 | AdvPrec1 45.930 | AdvPrec5 92.853 | Reg term: 0.0 \n",
      "Train Epoch:156 | Loss 0.1775 | AdvPrec1 92.910 | AdvPrec5 99.977 | Reg term: 0.\n",
      "Train Epoch:157 | Loss 0.1716 | AdvPrec1 93.173 | AdvPrec5 99.964 | Reg term: 0.\n",
      "Train Epoch:158 | Loss 0.1689 | AdvPrec1 93.326 | AdvPrec5 99.969 | Reg term: 0.\n",
      "Train Epoch:159 | Loss 0.1660 | AdvPrec1 93.459 | AdvPrec5 99.970 | Reg term: 0.\n",
      "Train Epoch:160 | Loss 0.1657 | AdvPrec1 93.373 | AdvPrec5 99.977 | Reg term: 0.\n",
      "Val Epoch:160 | Loss 1.0871 | NatPrec1 76.098 | NatPrec5 97.619 | Reg term: 0.0 \n",
      "Val Epoch:160 | Loss 3.6069 | AdvPrec1 45.584 | AdvPrec5 92.834 | Reg term: 0.0 \n",
      "Train Epoch:161 | Loss 0.1609 | AdvPrec1 93.542 | AdvPrec5 99.977 | Reg term: 0.\n",
      "Train Epoch:162 | Loss 0.1595 | AdvPrec1 93.657 | AdvPrec5 99.982 | Reg term: 0.\n",
      "Train Epoch:163 | Loss 0.1583 | AdvPrec1 93.741 | AdvPrec5 99.971 | Reg term: 0.\n",
      "Train Epoch:164 | Loss 0.1537 | AdvPrec1 93.883 | AdvPrec5 99.974 | Reg term: 0.\n",
      "Train Epoch:165 | Loss 0.1547 | AdvPrec1 93.926 | AdvPrec5 99.978 | Reg term: 0.\n",
      "Val Epoch:165 | Loss 1.0984 | NatPrec1 76.019 | NatPrec5 97.632 | Reg term: 0.0 \n",
      "Val Epoch:165 | Loss 3.6338 | AdvPrec1 45.682 | AdvPrec5 92.987 | Reg term: 0.0 \n",
      "Train Epoch:166 | Loss 0.1518 | AdvPrec1 93.947 | AdvPrec5 99.988 | Reg term: 0.\n",
      "Train Epoch:167 | Loss 0.1505 | AdvPrec1 94.084 | AdvPrec5 99.972 | Reg term: 0.\n",
      "Train Epoch:168 | Loss 0.1486 | AdvPrec1 94.103 | AdvPrec5 99.980 | Reg term: 0.\n",
      "Train Epoch:169 | Loss 0.1454 | AdvPrec1 94.181 | AdvPrec5 99.974 | Reg term: 0.\n",
      "Train Epoch:170 | Loss 0.1449 | AdvPrec1 94.266 | AdvPrec5 99.984 | Reg term: 0.\n",
      "Val Epoch:170 | Loss 1.1167 | NatPrec1 76.043 | NatPrec5 97.541 | Reg term: 0.0 \n",
      "Val Epoch:170 | Loss 3.7400 | AdvPrec1 45.578 | AdvPrec5 92.704 | Reg term: 0.0 \n",
      "Train Epoch:171 | Loss 0.1431 | AdvPrec1 94.344 | AdvPrec5 99.981 | Reg term: 0.\n",
      "Train Epoch:172 | Loss 0.1426 | AdvPrec1 94.414 | AdvPrec5 99.984 | Reg term: 0.\n",
      "Train Epoch:173 | Loss 0.1418 | AdvPrec1 94.289 | AdvPrec5 99.984 | Reg term: 0.\n",
      "Train Epoch:174 | Loss 0.1375 | AdvPrec1 94.598 | AdvPrec5 99.982 | Reg term: 0.\n",
      "Train Epoch:175 | Loss 0.1371 | AdvPrec1 94.618 | AdvPrec5 99.972 | Reg term: 0.\n",
      "Val Epoch:175 | Loss 1.1193 | NatPrec1 76.012 | NatPrec5 97.578 | Reg term: 0.0 \n",
      "Val Epoch:175 | Loss 3.7283 | AdvPrec1 45.542 | AdvPrec5 93.051 | Reg term: 0.0 \n",
      "Train Epoch:176 | Loss 0.1357 | AdvPrec1 94.660 | AdvPrec5 99.979 | Reg term: 0.\n",
      "Train Epoch:177 | Loss 0.1363 | AdvPrec1 94.584 | AdvPrec5 99.981 | Reg term: 0.\n",
      "Train Epoch:178 | Loss 0.1334 | AdvPrec1 94.738 | AdvPrec5 99.981 | Reg term: 0.\n",
      "Train Epoch:179 | Loss 0.1326 | AdvPrec1 94.683 | AdvPrec5 99.984 | Reg term: 0.\n",
      "Train Epoch:180 | Loss 0.1316 | AdvPrec1 94.739 | AdvPrec5 99.990 | Reg term: 0.\n",
      "Val Epoch:180 | Loss 1.1427 | NatPrec1 75.898 | NatPrec5 97.520 | Reg term: 0.0 \n",
      "Val Epoch:180 | Loss 3.8510 | AdvPrec1 45.239 | AdvPrec5 92.846 | Reg term: 0.0 \n",
      "Train Epoch:181 | Loss 0.1306 | AdvPrec1 94.866 | AdvPrec5 99.989 | Reg term: 0.\n",
      "Train Epoch:182 | Loss 0.1306 | AdvPrec1 94.847 | AdvPrec5 99.979 | Reg term: 0.\n",
      "Train Epoch:183 | Loss 0.1277 | AdvPrec1 94.921 | AdvPrec5 99.987 | Reg term: 0.\n",
      "Train Epoch:184 | Loss 0.1275 | AdvPrec1 94.989 | AdvPrec5 99.980 | Reg term: 0.\n",
      "Train Epoch:185 | Loss 0.1262 | AdvPrec1 95.050 | AdvPrec5 99.980 | Reg term: 0.\n",
      "Val Epoch:185 | Loss 1.1434 | NatPrec1 75.879 | NatPrec5 97.524 | Reg term: 0.0 \n",
      "Val Epoch:185 | Loss 3.8474 | AdvPrec1 45.349 | AdvPrec5 93.008 | Reg term: 0.0 \n",
      "Train Epoch:186 | Loss 0.1253 | AdvPrec1 95.069 | AdvPrec5 99.986 | Reg term: 0.\n",
      "Train Epoch:187 | Loss 0.1238 | AdvPrec1 95.014 | AdvPrec5 99.980 | Reg term: 0.\n",
      "Train Epoch:188 | Loss 0.1230 | AdvPrec1 95.129 | AdvPrec5 99.983 | Reg term: 0.\n",
      "Train Epoch:189 | Loss 0.1239 | AdvPrec1 95.014 | AdvPrec5 99.984 | Reg term: 0.\n",
      "Train Epoch:190 | Loss 0.1215 | AdvPrec1 95.204 | AdvPrec5 99.991 | Reg term: 0.\n",
      "Val Epoch:190 | Loss 1.1533 | NatPrec1 75.948 | NatPrec5 97.544 | Reg term: 0.0 \n",
      "Val Epoch:190 | Loss 3.9151 | AdvPrec1 45.500 | AdvPrec5 92.970 | Reg term: 0.0 \n",
      "Train Epoch:191 | Loss 0.1183 | AdvPrec1 95.253 | AdvPrec5 99.989 | Reg term: 0.\n",
      "Train Epoch:192 | Loss 0.1180 | AdvPrec1 95.339 | AdvPrec5 99.990 | Reg term: 0.\n",
      "Train Epoch:193 | Loss 0.1161 | AdvPrec1 95.394 | AdvPrec5 99.983 | Reg term: 0.\n",
      "Train Epoch:194 | Loss 0.1155 | AdvPrec1 95.423 | AdvPrec5 99.992 | Reg term: 0.\n",
      "Train Epoch:195 | Loss 0.1173 | AdvPrec1 95.399 | AdvPrec5 99.987 | Reg term: 0.\n",
      "Val Epoch:195 | Loss 1.1781 | NatPrec1 75.890 | NatPrec5 97.508 | Reg term: 0.0 \n",
      "Val Epoch:195 | Loss 3.9855 | AdvPrec1 45.359 | AdvPrec5 92.989 | Reg term: 0.0 \n",
      "Train Epoch:196 | Loss 0.1157 | AdvPrec1 95.449 | AdvPrec5 99.984 | Reg term: 0.\n",
      "Train Epoch:197 | Loss 0.1142 | AdvPrec1 95.511 | AdvPrec5 99.989 | Reg term: 0.\n",
      "Train Epoch:198 | Loss 0.1144 | AdvPrec1 95.491 | AdvPrec5 99.990 | Reg term: 0.\n",
      "Train Epoch:199 | Loss 0.1140 | AdvPrec1 95.510 | AdvPrec5 99.982 | Reg term: 0.\n",
      "Val Epoch:199 | Loss 1.1958 | NatPrec1 75.617 | NatPrec5 97.523 | Reg term: 0.0 \n",
      "Val Epoch:199 | Loss 4.0606 | AdvPrec1 44.871 | AdvPrec5 93.139 | Reg term: 0.0 \n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "train.train_model(train_args, m, (train_loader, val_loader), store=out_store)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename to   CINIC-50Store-ADV"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
